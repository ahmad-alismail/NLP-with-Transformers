{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n!git clone https://github.com/nlp-with-transformers/notebooks.git\n%cd notebooks\nfrom install import *\ninstall_requirements()","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:30:59.459752Z","iopub.execute_input":"2023-03-03T12:30:59.460214Z","iopub.status.idle":"2023-03-03T12:32:24.459674Z","shell.execute_reply.started":"2023-03-03T12:30:59.460174Z","shell.execute_reply":"2023-03-03T12:32:24.458038Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'notebooks'...\nremote: Enumerating objects: 515, done.\u001b[K\nremote: Counting objects: 100% (515/515), done.\u001b[K\nremote: Compressing objects: 100% (278/278), done.\u001b[K\nremote: Total 515 (delta 245), reused 479 (delta 231), pack-reused 0\u001b[K\nReceiving objects: 100% (515/515), 29.39 MiB | 16.67 MiB/s, done.\nResolving deltas: 100% (245/245), done.\n/kaggle/working/notebooks\n⏳ Installing base requirements ...\n✅ Base requirements installed!\n⏳ Installing Git LFS ...\n✅ Git LFS installed!\n","output_type":"stream"}]},{"cell_type":"code","source":"#hide\nfrom utils import *\nsetup_chapter()","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:32:24.462497Z","iopub.execute_input":"2023-03-03T12:32:24.462901Z","iopub.status.idle":"2023-03-03T12:32:28.008675Z","shell.execute_reply.started":"2023-03-03T12:32:24.462854Z","shell.execute_reply":"2023-03-03T12:32:28.007555Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"No GPU was detected! This notebook can be *very* slow without a GPU 🐢\nGo to Settings > Accelerator and select GPU.\nUsing transformers v4.11.3\nUsing datasets v1.16.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multilingual Named Entity Recognition","metadata":{}},{"cell_type":"markdown","source":"## The Dataset","metadata":{}},{"cell_type":"code","source":"#id jeff-dean-ner\n#caption An example of a sequence annotated with named entities\n#hide_input\nimport pandas as pd\ntoks = \"Jeff Dean is a computer scientist at Google in California\".split()\nlbls = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\ndf = pd.DataFrame(data=[toks, lbls], index=['Tokens', 'Tags'])\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:32:28.009899Z","iopub.execute_input":"2023-03-03T12:32:28.010570Z","iopub.status.idle":"2023-03-03T12:32:28.046175Z","shell.execute_reply.started":"2023-03-03T12:32:28.010533Z","shell.execute_reply":"2023-03-03T12:32:28.044786Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            0      1   2  3         4          5   6       7   8           9\nTokens   Jeff   Dean  is  a  computer  scientist  at  Google  in  California\nTags    B-PER  I-PER   O  O         O          O   O   B-ORG   O       B-LOC","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>Jeff</td>\n      <td>Dean</td>\n      <td>is</td>\n      <td>a</td>\n      <td>computer</td>\n      <td>scientist</td>\n      <td>at</td>\n      <td>Google</td>\n      <td>in</td>\n      <td>California</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>B-PER</td>\n      <td>I-PER</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-ORG</td>\n      <td>O</td>\n      <td>B-LOC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* To load one of the `PAN-X` subsets in `XTREME`, we'll nee to know which data configuration to pass the `load_dataset()` function. Let's use the `get_dataset_config_names()` function to find out which subsets are available:","metadata":{}},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:32:28.048833Z","iopub.execute_input":"2023-03-03T12:32:28.049223Z","iopub.status.idle":"2023-03-03T12:32:29.574078Z","shell.execute_reply.started":"2023-03-03T12:32:28.049187Z","shell.execute_reply":"2023-03-03T12:32:29.573091Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/9.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a7e0d10610438e86a83b8db198c84a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/23.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2a25ab0f5774bd9bf8ea7745bdf0edd"}},"metadata":{}},{"name":"stdout","text":"XTREME has 183 configurations\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* A lot of configuration! Let's narrow our search:","metadata":{}},{"cell_type":"code","source":"# finda configurations starts with PAN\npanx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\npanx_subsets[:3]","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:32:29.575127Z","iopub.execute_input":"2023-03-03T12:32:29.575463Z","iopub.status.idle":"2023-03-03T12:32:29.584845Z","shell.execute_reply.started":"2023-03-03T12:32:29.575431Z","shell.execute_reply":"2023-03-03T12:32:29.583410Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"},"metadata":{}}]},{"cell_type":"code","source":"# load the German corpus\nfrom datasets import load_dataset\n\nload_dataset(\"xtreme\", name=\"PAN-X.de\")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:32:29.586486Z","iopub.execute_input":"2023-03-03T12:32:29.586877Z","iopub.status.idle":"2023-03-03T12:32:49.557772Z","shell.execute_reply.started":"2023-03-03T12:32:29.586826Z","shell.execute_reply":"2023-03-03T12:32:49.556568Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.de (download: 223.17 MiB, generated: 9.08 MiB, post-processed: Unknown size, total: 232.25 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/234M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"027f553f5f064bb8a2c2942f31dba9f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b51582da50b84e4298b18e20ae3a56d0"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    train: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"* Lets create a realistic Swiss corpus. That is, we'll sample German, French, Italian, and English corpora from `PAN-X` according to their spoken proportions:","metadata":{}},{"cell_type":"code","source":"# hide_output\nfrom collections import defaultdict # same as dictionary but it never raises a KeyError\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n# Return a DatasetDict if a key doesn't exist\npanx_ch = defaultdict(DatasetDict) # Multilingual corpus\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    mono_corpus = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    \n    # Shuffle and downsample each split according to spoken proportion\n    for split in mono_corpus:               # train, test, validation\n        panx_ch[lang][split] = (\n            mono_corpus[split]\n            # avoid bias in dataset spolits\n            .shuffle(seed=0)\n            # downsample each corpus according to the values in fracs\n            .select(range(int(frac * mono_corpus[split].num_rows))))","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:32:49.559444Z","iopub.execute_input":"2023-03-03T12:32:49.559924Z","iopub.status.idle":"2023-03-03T12:33:14.935854Z","shell.execute_reply.started":"2023-03-03T12:32:49.559746Z","shell.execute_reply":"2023-03-03T12:33:14.934217Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1c1ce921274859923af7412d69ee66"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.fr (download: 223.17 MiB, generated: 6.37 MiB, post-processed: Unknown size, total: 229.53 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e0411c1f18b45af9602ac9b17b19482"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.it (download: 223.17 MiB, generated: 7.35 MiB, post-processed: Unknown size, total: 230.52 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab37ff7e00648b4875750a54a43a131"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.en (download: 223.17 MiB, generated: 7.30 MiB, post-processed: Unknown size, total: 230.47 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c3ff55aca64b18af2c9b5a9e46d26f"}},"metadata":{}}]},{"cell_type":"code","source":"{lang: panx_ch[lang][\"train\"].num_rows for lang in langs}","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:14.938085Z","iopub.execute_input":"2023-03-03T12:33:14.939566Z","iopub.status.idle":"2023-03-03T12:33:14.949953Z","shell.execute_reply.started":"2023-03-03T12:33:14.939500Z","shell.execute_reply":"2023-03-03T12:33:14.948645Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'de': 12580, 'fr': 4580, 'it': 1680, 'en': 1180}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n# number of example per language in the training split in the multilingual corpus\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n             index=[\"Number of training examples\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:14.951896Z","iopub.execute_input":"2023-03-03T12:33:14.952713Z","iopub.status.idle":"2023-03-03T12:33:15.361185Z","shell.execute_reply.started":"2023-03-03T12:33:14.952662Z","shell.execute_reply":"2023-03-03T12:33:15.360300Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                de    fr    it    en\nNumber of training examples  12580  4580  1680  1180","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de</th>\n      <th>fr</th>\n      <th>it</th>\n      <th>en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of training examples</th>\n      <td>12580</td>\n      <td>4580</td>\n      <td>1680</td>\n      <td>1180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"element = panx_ch[\"de\"][\"train\"][0]\nfor key, value in element.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:15.365152Z","iopub.execute_input":"2023-03-03T12:33:15.365762Z","iopub.status.idle":"2023-03-03T12:33:15.431563Z","shell.execute_reply.started":"2023-03-03T12:33:15.365724Z","shell.execute_reply":"2023-03-03T12:33:15.430547Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der',\n'polnischen', 'Woiwodschaft', 'Pommern', '.']\nner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\nlangs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n","output_type":"stream"}]},{"cell_type":"code","source":"for key, value in panx_ch[\"de\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:15.434981Z","iopub.execute_input":"2023-03-03T12:33:15.435736Z","iopub.status.idle":"2023-03-03T12:33:15.506810Z","shell.execute_reply.started":"2023-03-03T12:33:15.435676Z","shell.execute_reply":"2023-03-03T12:33:15.505322Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\nner_tags: Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER',\n'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None),\nlength=-1, id=None)\nlangs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:15.509191Z","iopub.execute_input":"2023-03-03T12:33:15.510157Z","iopub.status.idle":"2023-03-03T12:33:15.584048Z","shell.execute_reply.started":"2023-03-03T12:33:15.510098Z","shell.execute_reply":"2023-03-03T12:33:15.583070Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n'B-LOC', 'I-LOC'], names_file=None, id=None)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* We can use `ClassLabel.int2str()` method to create a new column in our training set with class names for each tag. We'll use the `map()` method to return a `dict` with keys corresponding to the new column name and the value as a `list` of class names:","metadata":{}},{"cell_type":"code","source":"# hide_output\ndef create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:15.585515Z","iopub.execute_input":"2023-03-03T12:33:15.585872Z","iopub.status.idle":"2023-03-03T12:33:19.810069Z","shell.execute_reply.started":"2023-03-03T12:33:15.585838Z","shell.execute_reply":"2023-03-03T12:33:19.808870Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e1f5cc109c4a63a654cc9219bf96ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8844539d11694351bca793d018854717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12580 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd0bb70754c47c799df2e583ea6a91c"}},"metadata":{}}]},{"cell_type":"code","source":"# hide_output\nde_example = panx_de[\"train\"][0]\npd.DataFrame(data=[de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\nindex=['Tokens', 'Tags'])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:19.811681Z","iopub.execute_input":"2023-03-03T12:33:19.812051Z","iopub.status.idle":"2023-03-03T12:33:19.830715Z","shell.execute_reply.started":"2023-03-03T12:33:19.812016Z","shell.execute_reply":"2023-03-03T12:33:19.829442Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"           0           1   2    3         4      5   6    7           8   \\\nTokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \nTags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n\n                  9        10 11  \nTokens  Woiwodschaft  Pommern  .  \nTags           B-LOC    I-LOC  O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>2.000</td>\n      <td>Einwohnern</td>\n      <td>an</td>\n      <td>der</td>\n      <td>Danziger</td>\n      <td>Bucht</td>\n      <td>in</td>\n      <td>der</td>\n      <td>polnischen</td>\n      <td>Woiwodschaft</td>\n      <td>Pommern</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Let's calculate the frequencies of each entity across each split:","metadata":{}},{"cell_type":"markdown","source":"````python\npanx_de.items()\n> dict_items([('validation', Dataset({\n    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n    num_rows: 6290\n})), ('test', Dataset({\n    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n    num_rows: 6290\n})), ('train', Dataset({\n    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n    num_rows: 12580\n}))])\n````","metadata":{}},{"cell_type":"code","source":"# import Counter class from collections module\nfrom collections import Counter   \n\n# create defaultdict to store frequency of each NER tag type for each split\nsplit2freqs = defaultdict(Counter)\n\n# iterate over splits in the mulitlingual corpus\nfor split, dataset in panx_de.items():\n    # iterate over rows in current split\n    for row in dataset[\"ner_tags_str\"]:\n         # iterate over NER tags in current row\n        for tag in row:                            \n            if tag.startswith(\"B\"):                   # if tag is a beginning tag\n                tag_type = tag.split(\"-\")[1]          # extract tag type\n                split2freqs[split][tag_type] += 1     # increment count of tag type in current split\n                \n# create DataFrame from split2freqs defaultdict and display it\npd.DataFrame.from_dict(split2freqs, orient=\"index\")  ","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:19.832502Z","iopub.execute_input":"2023-03-03T12:33:19.833542Z","iopub.status.idle":"2023-03-03T12:33:20.492945Z","shell.execute_reply.started":"2023-03-03T12:33:19.833480Z","shell.execute_reply":"2023-03-03T12:33:20.491621Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"             ORG   LOC   PER\nvalidation  2683  3172  2893\ntest        2573  3180  3071\ntrain       5366  6186  5810","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ORG</th>\n      <th>LOC</th>\n      <th>PER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>validation</th>\n      <td>2683</td>\n      <td>3172</td>\n      <td>2893</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>2573</td>\n      <td>3180</td>\n      <td>3071</td>\n    </tr>\n    <tr>\n      <th>train</th>\n      <td>5366</td>\n      <td>6186</td>\n      <td>5810</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Multilingual Transformers","metadata":{}},{"cell_type":"markdown","source":"* Multilingual transformers invole similar architecture and training proceduers as their monolingual counterparts, except using multilingual corpus for pre training.\n* They can generalize well across languages for variety of downstream tasks.\n* Multilingual transformers are usually evaluated in three different ways:\n    * `en`: **Fine-tune** on the English training data and then **evaluate** on each language's test set.\n    * `each`: **Fine-tune** and **evaluate** on monolingual test data to measure per-language performance.\n    * `all`: **Fine-tune** on all the training data to **evaluate** on all on each language's test set.\n","metadata":{}},{"cell_type":"markdown","source":"## XLM-RoBERTa (aka, XLM-R)\n * We'll select XLM-RoBERTa as our multilingual transformer\n * RoBERTa part of XLM-RoBERTa means the the pretraining approach is the same as for monolingual RoBERTa that remove next sentence prediction task.\n * XLM-RoBERTa has a large vocabulary with 250000 tokens\n * XLM-RoBERTa uses `SentencePiece` instead of `WordPiece` to tokenize the raw text directly","metadata":{}},{"cell_type":"markdown","source":"## A Closer Look at Tokenization\nLet's compare BERT's `WordPiece` with XLM-RoBERTa's `SentencePiece` tokenizers:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbert_model_name = \"bert-base-cased\"\nxlmr_model_name = \"xlm-roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:20.494913Z","iopub.execute_input":"2023-03-03T12:33:20.495394Z","iopub.status.idle":"2023-03-03T12:33:42.941314Z","shell.execute_reply.started":"2023-03-03T12:33:20.495307Z","shell.execute_reply":"2023-03-03T12:33:42.939823Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca81048841a044e29b7ebba6e5d45fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"659b97a1215f4504a1091cd0ba8dfa54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd85bd5828e4080af1d09079fa8db59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eeb166a9c594663b30b17875cbef4e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75936a4091724decb53f360a23ebd8d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e87b904841435a8c456d00edd0d058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54060443a324499f9d06fc45ad702772"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"Jack Sparrow loves New York!\"\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:42.943148Z","iopub.execute_input":"2023-03-03T12:33:42.945596Z","iopub.status.idle":"2023-03-03T12:33:42.954196Z","shell.execute_reply.started":"2023-03-03T12:33:42.945528Z","shell.execute_reply":"2023-03-03T12:33:42.953198Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\ndf = pd.DataFrame(data=[bert_tokens, xlmr_tokens], index=[\"BERT\", \"XLM-R\"])\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:42.955812Z","iopub.execute_input":"2023-03-03T12:33:42.956506Z","iopub.status.idle":"2023-03-03T12:33:42.981759Z","shell.execute_reply.started":"2023-03-03T12:33:42.956467Z","shell.execute_reply":"2023-03-03T12:33:42.980458Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"           0      1      2       3      4    5     6      7      8     9\nBERT   [CLS]   Jack    Spa  ##rrow  loves  New  York      !  [SEP]  None\nXLM-R    <s>  ▁Jack  ▁Spar     row  ▁love    s  ▁New  ▁York      !  </s>","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BERT</th>\n      <td>[CLS]</td>\n      <td>Jack</td>\n      <td>Spa</td>\n      <td>##rrow</td>\n      <td>loves</td>\n      <td>New</td>\n      <td>York</td>\n      <td>!</td>\n      <td>[SEP]</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>XLM-R</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* XLM-RoBERTa uses `<s>` and `<\\s>` instead of `[CLS]` and `[SEP]` to denote the start and end of a sequence\n* Whitespace is represented with the Unicode symbol **`__`**\n* Subwords in BERT's tokenizer starts with `##`\n\n\nLet's delve deeper to gain a better understanding of how the tokenizer works in general. Later, we will see what makes the `SentencePiece` tokenizer so special?\n","metadata":{}},{"cell_type":"markdown","source":"### The Tokenizer Pipeline","metadata":{}},{"cell_type":"markdown","source":"Tokenization is not just a single operation that transforms strings to integers we can pass through the model. Tokenization consists usually of four steps as shown below:","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Tokenizer pipeline\" caption=\"The steps in the tokenization pipeline\" src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter04_tokenizer-pipeline.png\" id=\"toknizer-pipeline\"/>","metadata":{}},{"cell_type":"markdown","source":"1. **Normalization**: A set of operations are applied to raw string to make it cleaner. For example, stripping whitespace, removing accented characters, unicode normalization, and lowercasing. For example, `\"jack sparrow loves new york!\"`\n2. **Pretokenization**: In this step we will split our text into smaller objects (i.e., words). For example `[\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]`. In the next step, these words are split into subwords with Byte-Pair Encoding (BPE) or Unigram algorithms.\n3. **Tokenizer model**: the tokenizer applies a **subword splitting model** (BPE, Unigram, WordPiece). This part of the pipleling needs to be trained on your corpus. \n> Note: The output of the subword splitting model is a **list of integers (Input IDs)**. For illustration, the output in strings would look like `[jack, spa,rrow, loves, new, york, !]`\n4. **Postprocessing**: this step add some transformation on the list of tokens, e.g., adding special tokens at the beginning or end of the input sequence of token indices. The output would be like `[[CLS], jack, spa, rrow, loves, new, york, !, [SEP]]`\n\nThe question that arises now is, what makes the `SentencePiece` tokenizer so special?","metadata":{}},{"cell_type":"markdown","source":"### The SentencePiece Tokenizer\n* The `SentencePiece` tokenizer is based on a type of subword segemntation called **Unigram** and encodes each input text as a sequence of **Unicode characters**. This feature is very useful for multilingual corpora since it allows `SentencePiece` to be agnostic about accents, punctuation, and the fact that many language (e.g., Japanese) do NOT have whitespace characters. \n* Another special feature of `SentencePiece` is the whitespece is assigned to the Unicode Sympol `U+2591` or the **`__`** character. This enables `SentencePiece`  to detokenize a sequence without relying on language-specific pretokenization.\n* In In our example from the previous section, e can see that `WordPiece` has LOST the information that there is NO whitespace between \"York\" and \"!\". By contrast, `SentencePiece` preserves the whitespace in the tokenized text so we can convert back the raw text without ambiguity:\nI","metadata":{}},{"cell_type":"code","source":"\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:42.983130Z","iopub.execute_input":"2023-03-03T12:33:42.983711Z","iopub.status.idle":"2023-03-03T12:33:42.990674Z","shell.execute_reply.started":"2023-03-03T12:33:42.983675Z","shell.execute_reply":"2023-03-03T12:33:42.989668Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'<s> Jack Sparrow loves New York!</s>'"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we understand how `SentencePiece` works, let's see how we can encode our simple example in a form of suitable for NER. The first thing to do is **load** the *pretrained model* with the *token classification head*. But **instead** of loading this head directoly from  🤗 Transformers, we will **build** it ourselves!","metadata":{}},{"cell_type":"markdown","source":"## Transformers for Named Entity Recognition\n","metadata":{}},{"cell_type":"markdown","source":"### Architecture of a transformer encoder for classification\nFor sequence classification,  BERT and other encoder-only trasnsformers use the special `[CLS]` token represent an entire sequence of text and feed it through fully connected layer to get the distrubtion.","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Architecture of a transformer encoder for classification.\" caption=\"Fine-tuning an encoder-based transformer for sequence classification\" src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter04_clf-architecture.png\" id=\"clf-arch\"/>","metadata":{}},{"cell_type":"markdown","source":"### Architecture of a transformer encoder for named entity recognition. The wide linear layer shows that the same linear layer is applied to all hidden states.\nFor token classification task, BERT and other encoder-only transformers feed the representation/hidden state of each token into the fully connected layer to get the probability for each class label. ","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Architecture of a transformer encoder for named entity recognition. The wide linear layer shows that the same linear layer is applied to all hidden states.\" caption=\"Fine-tuning an encoder-based transformer for named entity recognition\" src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter04_ner-architecture.png\" id=\"ner-arch\"/>","metadata":{}},{"cell_type":"markdown","source":"As mentioned before, we will use 🤗 Transformers to load the pretrained model and build a specific classification head for NER. Let's understand the anatomy of 🤗 Transformers model class in huggingface first:","metadata":{}},{"cell_type":"markdown","source":"## The Anatomy of the Transformers Model Class","metadata":{}},{"cell_type":"markdown","source":"* 🤗 Transformers has classes for each architecture and task. The model classes are named according to `<ModelName>For<Task>` convention, or `AutoModelFor<Task>` when using `AutoModel` classes. However, the **main limitation** using this approach is the lack of models for our specific needs. Therefore, 🤗 Transformers is designed to enable you to easily extend existing models for your specific use case by loading weights from a pretrained model, and build your task-specific classification head.\n\nIn the next section, we'll see how we can implement our own custom model.","metadata":{}},{"cell_type":"markdown","source":"### Bodies and Heads","metadata":{}},{"cell_type":"markdown","source":"The main concept that makes 🤗 Transformers so versatile is the split of the **architecture** into a **body** and **head**. This separation of bodies and heads allows us to build a custom head for any task and just mount it on top of a pretrained model.\n* **Model head**: is the last layer and represents the **task-specific** layer.\n* **Model body**: is the rest of the model that includes the token **embeddings** and **transformer layers**. The body is **task-agnostic**. It implemented in class such as `BertModel` or `GPT2Model` that return the **hidden states** of the last layer.\n> NOTE: Task-specific models such as `BertForSequenceClassification` use the **base model (i.e., body)** and add the necessary **head** on the top of hidden states as shown below:\n","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"bert-body-head\" caption=\"The `BertModel` class only contains the body of the model, while the `BertFor&lt;Task&gt;` classes combine the body with a dedicated head for a given task\" src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter04_bert-body-head.png\" id=\"bert-body-head\"/>","metadata":{}},{"cell_type":"markdown","source":"### Creating a Custom Model for Token Classification","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n\n# Create a class for a custom model, which inherit from RobertaPreTrainedModel since we want to use the weights of a pretained model in the body of a custom model\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    # Common practice in 🤗 Transformers \n    # ensures that the standard XLMRoberta settings are used when initialize a new model\n    # to change the defualt parametres by overwriting the defualt settings in config\n    config_class = XLMRobertaConfig\n\n    # initialize the model\n    def __init__(self, config):\n        # call the initialization function of the parent class (RobertaPreTrainedModel)\n        super().__init__(config)              # config to overwrite default settings in config?\n        self.num_labels = config.num_labels   # number of classes to predict\n\n        # Load model BODY\n        self.roberta = RobertaModel(config, add_pooling_layer=False) # returns all hidden states not just [CLS]\n        \n        # Set up token CLASSIFICATION HEAD\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)             \n        self.classifier = nn.Linear(config.hidden_size, config.num_labels) # linear transformation layer takes (batch_size, sequence_length, hidden_size) \n                                                                           # to produce output tensor of shape (batch_size, sequence_length, num_labels)\n                                                                           # which can be interpreted as probability distribution over the labels for each token in the input sequence.\n        \n        # Load the pretrained weights for the model body and \n        # ... randomly initialize weights of token classification head\n        self.init_weights()\n\n    # define the forward pass\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                labels=None, **kwargs):\n        # Feed the data through model BODY to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n                               token_type_ids=token_type_ids, **kwargs)\n        \n        # Apply classifier to encoder representation \n        sequence_output = self.dropout(outputs[0]) # apply dropout to the first element of output tensor, i.e., last_hidden_state\n        logits = self.classifier(sequence_output)  # apply the linear transformation to get the logits (i.e., raw output of the model)\n        # Calculate losses if labels are provided\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) # apply cross entropy function on flattend logits and flattend labels\n        # Return model output object\n        return TokenClassifierOutput(loss=loss, logits=logits, \n                                     hidden_states=outputs.hidden_states, \n                                     attentions=outputs.attentions)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:42.992242Z","iopub.execute_input":"2023-03-03T12:33:42.995424Z","iopub.status.idle":"2023-03-03T12:33:43.024638Z","shell.execute_reply.started":"2023-03-03T12:33:42.995353Z","shell.execute_reply":"2023-03-03T12:33:43.023594Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Loading a Custom Model","metadata":{}},{"cell_type":"markdown","source":"Before loading our custom token classification model, we have to provide some additional information beyond the model name: \n* The tags we will use to label each entity\n* The mapping of each tag to an ID and vice versa\n\nAll of this information can be derived from our `tags` variable:\n````python\nprint(tags)\n> ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None)\n````","metadata":{}},{"cell_type":"code","source":"# mapping of each tag to an ID and vice versa\nindex2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:43.026313Z","iopub.execute_input":"2023-03-03T12:33:43.026888Z","iopub.status.idle":"2023-03-03T12:33:43.032046Z","shell.execute_reply.started":"2023-03-03T12:33:43.026853Z","shell.execute_reply":"2023-03-03T12:33:43.030804Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"````python\nprint(index2tag)\n> {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n\nprint(tag2index)\n> {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n````","metadata":{}},{"cell_type":"markdown","source":"* We'll store these mappings and `tags.num_classes` attribute in `AutoConfig` object. Passing keyword arguments to the `from_pretrained()` method **overrides** the defualt values:\n\n> `AutoConfig` class contains the blueprint of a model's architecture. When we load a model with `AutoModel.from_pretrained(model_ckpt)`, the configuration file associated with that model is downloaded **automatically**. However, if we want to **modify** something like the number of classes or label names, then we can **load** the configuration first with the **parameters** we would like to **customize**.","metadata":{}},{"cell_type":"code","source":"# store these mapping and tags.num_classes attribute in Autoconfig object\n# passing arguments to from_pretrained()\nfrom transformers import AutoConfig\n\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name,                 # 'xlm-roberta-base'\n                                         num_labels=tags.num_classes,     # 7 \n                                         id2label=index2tag, \n                                         label2id=tag2index)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:43.033473Z","iopub.execute_input":"2023-03-03T12:33:43.034269Z","iopub.status.idle":"2023-03-03T12:33:43.504684Z","shell.execute_reply.started":"2023-03-03T12:33:43.034233Z","shell.execute_reply":"2023-03-03T12:33:43.503610Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"* Now, we can load the model weights as usual with the `from_pretrained()` function with the additional `config` argument:\n> NOTE that we did NOT implement loading pre-trained weights in our custom model class; we get this for free by inheriting from `RobertaPreTrainedModel`","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nxlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:33:43.506091Z","iopub.execute_input":"2023-03-03T12:33:43.506428Z","iopub.status.idle":"2023-03-03T12:34:40.982636Z","shell.execute_reply.started":"2023-03-03T12:33:43.506385Z","shell.execute_reply":"2023-03-03T12:34:40.981465Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8352fe1a66864522843e03fd04e85f9e"}},"metadata":{}}]},{"cell_type":"code","source":"# xlmr_model","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:34:40.984193Z","iopub.execute_input":"2023-03-03T12:34:40.984805Z","iopub.status.idle":"2023-03-03T12:34:40.990157Z","shell.execute_reply.started":"2023-03-03T12:34:40.984764Z","shell.execute_reply":"2023-03-03T12:34:40.987460Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"* As a quick check that we have initialized the tokenizer and the model correctly, let's test the predictions on the following sentence:","metadata":{}},{"cell_type":"markdown","source":"````python\ntext = \"Jack Sparrow loves New York!\"\n````","metadata":{}},{"cell_type":"code","source":"# hide_output\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\npd.DataFrame(data=[xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:34:40.992282Z","iopub.execute_input":"2023-03-03T12:34:40.993064Z","iopub.status.idle":"2023-03-03T12:34:41.020022Z","shell.execute_reply.started":"2023-03-03T12:34:40.993014Z","shell.execute_reply":"2023-03-03T12:34:41.018705Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"             0      1      2      3      4  5     6      7   8     9\nTokens     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\nInput IDs    0  21763  37456  15555   5161  7  2356   5753  38     2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Input IDs</th>\n      <td>0</td>\n      <td>21763</td>\n      <td>37456</td>\n      <td>15555</td>\n      <td>5161</td>\n      <td>7</td>\n      <td>2356</td>\n      <td>5753</td>\n      <td>38</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<img alt=\"tensor dimensions\" caption=\"tensor dimensions\" src=\"https://github.com/ahmad-alismail/NLP-with-Transformers/blob/master/imges/softmax-pytroch.jpg?raw=true\" id=\"bert-body-head\"/>\n","metadata":{}},{"cell_type":"markdown","source":"````python\nprint(input_ids)\n> tensor([[    0, 21763, 37456, 15555,  5161,     7,  2356,  5753,    38,     2]])\n````","metadata":{}},{"cell_type":"code","source":"# pass the inputs to the model and extract the predictions\noutputs = xlmr_model(input_ids.to(device)).logits\npredictions = torch.argmax(outputs, dim=-1)   # -1 mean the last dimension \nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\nprint(f\"Shape of outputs: {outputs.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:34:41.021719Z","iopub.execute_input":"2023-03-03T12:34:41.022462Z","iopub.status.idle":"2023-03-03T12:34:41.231125Z","shell.execute_reply.started":"2023-03-03T12:34:41.022405Z","shell.execute_reply":"2023-03-03T12:34:41.229977Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Number of tokens in sequence: 10\nShape of outputs: torch.Size([1, 10, 7])\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:34:41.234494Z","iopub.execute_input":"2023-03-03T12:34:41.235534Z","iopub.status.idle":"2023-03-03T12:34:41.245962Z","shell.execute_reply.started":"2023-03-03T12:34:41.235488Z","shell.execute_reply":"2023-03-03T12:34:41.244808Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 2, 2, 2, 2, 2, 2, 2, 2, 2]])"},"metadata":{}}]},{"cell_type":"markdown","source":"````python\nprint(predictions)\n> tensor([[0, 0, 0, 0, 6, 0, 0, 0, 0, 0]])\n\nprint(tags.names)\n> ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n````","metadata":{}},{"cell_type":"code","source":"preds = [tags.names[p] for p in predictions[0].cpu().numpy()] # or index2tag[p]\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T12:34:41.252693Z","iopub.execute_input":"2023-03-03T12:34:41.253544Z","iopub.status.idle":"2023-03-03T12:34:41.272577Z","shell.execute_reply.started":"2023-03-03T12:34:41.253498Z","shell.execute_reply":"2023-03-03T12:34:41.271018Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"          0      1      2      3      4      5      6      7      8      9\nTokens  <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\nTags      O  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Our classification layer with random weights is still far from being accurate. Therefore, we have to fine-tune our custom model on labeled data. Before doing, let's wrap the preceding steps into a `tag_text` function that predicts the BIO tags for each token in an input text using a pre-trained model and returns them in a `DataFrame`.","metadata":{}},{"cell_type":"code","source":"def tag_text(text, tags, model, tokenizer):\n    # Get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # Encode the sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # Get predictions as distribution over 7 possible classes\n    outputs = model(input_ids)[0]                    # takes the first (and only) batch of outputs\n    # Take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=2)       # e.g., tensor([[0, 0, 0, 0, 6, 0, 0, 0, 0, 0]])\n    # Convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]   # or index2tag[p]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Before training, we have to perform the following steps:\n1. Tokenize the inputs\n2. Prepare labels\nLet's do it!","metadata":{}},{"cell_type":"markdown","source":"## Tokenizing Texts for NER","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"````python\nde_example\n> {\n 'tokens':       ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.'],\n 'ner_tags':     [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0],\n 'langs':        ['de','de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de'], \n 'ner_tags_str': ['O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-LOC', 'B-LOC', 'I-LOC','O']\n  }\n````","metadata":{}},{"cell_type":"code","source":"words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]","metadata":{"execution":{"iopub.status.busy":"2023-03-03T15:40:16.716334Z","iopub.execute_input":"2023-03-03T15:40:16.717505Z","iopub.status.idle":"2023-03-03T15:40:16.722613Z","shell.execute_reply.started":"2023-03-03T15:40:16.717459Z","shell.execute_reply":"2023-03-03T15:40:16.721434Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# tokenize each word given that the input sequence has been already split into words\ntokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T15:40:17.143263Z","iopub.execute_input":"2023-03-03T15:40:17.144428Z","iopub.status.idle":"2023-03-03T15:40:17.152774Z","shell.execute_reply.started":"2023-03-03T15:40:17.144345Z","shell.execute_reply":"2023-03-03T15:40:17.151307Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"````python\ntokenized_input\n> {\n 'input_ids':      [0, 70101, 176581, 19, 142, 122, 2290, 708, 1505, 18363, 18, 23, 122, 127474, 15439, 13787, 14, 15263, 18917, 663, 6947, 19, 6, 5, 2], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n}\n````","metadata":{}},{"cell_type":"code","source":"# return tokens after tokenization\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T15:40:17.970525Z","iopub.execute_input":"2023-03-03T15:40:17.971451Z","iopub.status.idle":"2023-03-03T15:40:17.977461Z","shell.execute_reply.started":"2023-03-03T15:40:17.971400Z","shell.execute_reply":"2023-03-03T15:40:17.976220Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"> NOTE: Whitespace is represented with the Unicode symbol **`__`**","metadata":{}},{"cell_type":"code","source":"pd.DataFrame([tokens], index=[\"Tokens\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T15:40:19.270118Z","iopub.execute_input":"2023-03-03T15:40:19.270587Z","iopub.status.idle":"2023-03-03T15:40:19.298046Z","shell.execute_reply.started":"2023-03-03T15:40:19.270541Z","shell.execute_reply":"2023-03-03T15:40:19.296662Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"         0       1           2  3    4     5     6   7    8      9   ...   15  \\\nTokens  <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  ▁Wo   \n\n       16   17      18   19    20 21 22 23    24  \nTokens  i  wod  schaft  ▁Po  mmer  n  ▁  .  </s>  \n\n[1 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* The tokenizer has split `Einwohnern` (English: residents) into `__Einwohner` and `n`. During the training, we will follow the convention that only the first subword `__Einwohner` should be tagged as begin of location entity `B-LOC`, we need to mask the second subword `n`. To acheive this, we'll `word_ids()` function that maps each token to the corresponding index in the **inital sequence** (i.e., `words`) for the tokenizer: \n> NOTE: **Special tokens** added by the tokenizer are mapped to `None` (e.g., `<s>`) and other tokens are mapped to the index of their corresponding word. Several tokens will be mapped to the same word index if they are parts of that word (e.g., `__Einwohner` and `n` have the same ID `1`).","metadata":{}},{"cell_type":"code","source":"word_ids = tokenized_input.word_ids()\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T15:40:26.719542Z","iopub.execute_input":"2023-03-03T15:40:26.720803Z","iopub.status.idle":"2023-03-03T15:40:26.744878Z","shell.execute_reply.started":"2023-03-03T15:40:26.720740Z","shell.execute_reply":"2023-03-03T15:40:26.743918Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"            0       1           2  3    4     5     6   7    8      9   ...  \\\nTokens     <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...   \nWord IDs  None       0           1  1    2     3     4   4    4      5  ...   \n\n           15 16   17      18   19    20  21  22  23    24  \nTokens    ▁Wo  i  wod  schaft  ▁Po  mmer   n   ▁   .  </s>  \nWord IDs    9  9    9       9   10    10  10  11  11  None  \n\n[2 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Word IDs</th>\n      <td>None</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>...</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>10</td>\n      <td>10</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Let's set `-100` as the label for the special tokens and the subwords we wish to **mask during training**:\n> The reason for choosing `-100` as mask ID is that `torch.nn.CrossEntropyLoss` class has an attribute `ignore_index` whose value is `-100`. This index is ignored during training.","metadata":{}},{"cell_type":"markdown","source":"````python\nword_ids\n> [None, 0, 1, 1, 2, 3, 4, 4, 4, 5, 5, 6, 7, 8, 8, 9, 9, 9, 9, 10, 10, 10, 11, 11, None]\n\nlabels\n> [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n````","metadata":{}},{"cell_type":"code","source":"previous_word_idx = None\nlabel_ids = []\n\n# transform word_ids into label_ids\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx]) # append the label at this index\n    previous_word_idx = word_idx\n\n# transform label_ids into tags/labels\n# if the label_id is not equal to -100 lookup the corresponding tag for that id and add it to the list\n# if the label_id is equal to -100 add \"IGN\"\nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T15:38:07.772926Z","iopub.execute_input":"2023-03-03T15:38:07.773406Z","iopub.status.idle":"2023-03-03T15:38:07.803354Z","shell.execute_reply.started":"2023-03-03T15:38:07.773355Z","shell.execute_reply":"2023-03-03T15:38:07.802104Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"             0       1           2     3    4     5      6     7     8   \\\nTokens      <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der   ▁Dan    zi   ger   \nWord IDs   None       0           1     1    2     3      4     4     4   \nLabel IDs  -100       0           0  -100    0     0      5  -100  -100   \nLabels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n\n              9   ...     15    16    17      18     19    20    21  22    23  \\\nTokens     ▁Buch  ...    ▁Wo     i   wod  schaft    ▁Po  mmer     n   ▁     .   \nWord IDs       5  ...      9     9     9       9     10    10    10  11    11   \nLabel IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \nLabels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n\n             24  \nTokens     </s>  \nWord IDs   None  \nLabel IDs  -100  \nLabels      IGN  \n\n[4 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Word IDs</th>\n      <td>None</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>...</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>10</td>\n      <td>10</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>Label IDs</th>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>...</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>-100</td>\n    </tr>\n    <tr>\n      <th>Labels</th>\n      <td>IGN</td>\n      <td>O</td>\n      <td>O</td>\n      <td>IGN</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>I-LOC</td>\n      <td>...</td>\n      <td>B-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>I-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>O</td>\n      <td>IGN</td>\n      <td>IGN</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Let's scale this out to the a split in the dataset by defining a single function that **encode a split of the dataset** (i.e., `train`, `test`, or `validation`):\n\n> NOTE: The input of this function is a nested list of tokens\n> \n````python\npanx_ch[\"de\"][\"validation\"][\"tokens\"]\n> [['Hama', '(', 'Unternehmen', ')'],\n   ['WEITERLEITUNG', 'Luzkyj', 'awtomobilnyj', 'sawod'],\n   ...\n   ...\n   ['entdeckt', 'und', 'gehört', 'der', 'Spektralklasse', 'L2', 'an', '.'],\n   ['**', \"'\", \"''\", 'Bretagne', \"''\", \"'\"]]\n````","metadata":{}},{"cell_type":"code","source":"# tokenize and align labels for all split of the dataset\ndef tokenize_and_align_labels(examples):\n    # Tokenize the input sequence using XLM-R tokenizer \n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], \n                                      truncation=True,          # cut off to a maximum length specifid with argument max_length\n                                      is_split_into_words=True) # input sequence has ALREADY been split into words\n    \n    # Empty list to store the aligned labels for each input sequence\n    labels = []\n    \n    # Iterate over all ner_tags lists in the input data\n    for idx, label in enumerate(examples[\"ner_tags\"]):          # idx 0 and label [3, 4, 4, 4]\n        # Get the word_ids for each token in the initial input sequence for the tokenizer\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)   # [None, 0, 0, 1, 2, 3, None]\n        \n        # Transform word_ids for an individual sequence into label_ids\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        # Append the label_ids list into labels list  [[-100, 3, -100, 4, 4, 4, -100], ...] \n        labels.append(label_ids)                     # ['ING', 'B-ORG', 'ING', 'I-ORG', 'I-ORG', 'I-ORG', 'ING'] (for illustration ONLY)\n    \n    # Add the the nested list of aligned labels to the tokenized_inputs structre\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have all the ingredients we need to encode each split, let's iterate over the **all the splits** (`train`, `test`, and `validation`) using the following function that returns a **modified version** of the dataset where the text has been tokenized and the labels have been aligned with the tokenized text: ","metadata":{}},{"cell_type":"code","source":"# Define a function to apply tokenize_and_align_labels function on each part of the corpus\ndef encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, \n                      batched=True,                  # process examples in batches to improve the efficiency\n                      remove_columns=['langs', 'ner_tags', 'tokens'])","metadata":{"execution":{"iopub.status.busy":"2023-03-03T18:08:10.361977Z","iopub.execute_input":"2023-03-03T18:08:10.362519Z","iopub.status.idle":"2023-03-03T18:08:10.369213Z","shell.execute_reply.started":"2023-03-03T18:08:10.362475Z","shell.execute_reply":"2023-03-03T18:08:10.367898Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# return a modified version of the dataset\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Measures","metadata":{}},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\nprint(classification_report(y_true, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n\n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n\n    return preds_list, labels_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning XLM-RoBERTa","metadata":{}},{"cell_type":"code","source":"# hide_output\nfrom transformers import TrainingArguments\n\nnum_epochs = 3\nbatch_size = 24\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\ntraining_args = TrainingArguments(\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n    logging_steps=logging_steps, push_to_hub=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, \n                                       eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_init():\n    return (XLMRobertaForTokenClassification\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\n            .to(device))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide\n%env TOKENIZERS_PARALLELISM=false","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfrom transformers import Trainer\n\ntrainer = Trainer(model_init=model_init, args=training_args, \n                  data_collator=data_collator, compute_metrics=compute_metrics,\n                  train_dataset=panx_de_encoded[\"train\"],\n                  eval_dataset=panx_de_encoded[\"validation\"], \n                  tokenizer=xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\ndf = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss', 'eval_f1']]\ndf = df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", \"eval_f1\":\"F1\"})\ndf['Epoch'] = df[\"Epoch\"].apply(lambda x: round(x))\ndf['Training Loss'] = df[\"Training Loss\"].ffill()\ndf[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\ndf.drop_duplicates()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error Analysis","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # Convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    # Pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        # Pass data through model  \n        output = trainer.model(input_ids, attention_mask)\n        # Logit.size: [batch_size, sequence_length, classes]\n        # Predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    # Calculate loss per token after flattening batch dimension with view\n    loss = cross_entropy(output.logits.view(-1, 7), \n                         labels.view(-1), reduction=\"none\")\n    # Unflatten batch dimension and convert to numpy array\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n\n    return {\"loss\":loss, \"predicted_label\": predicted_label}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nvalid_set = panx_de_encoded[\"validation\"]\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = valid_set.to_pandas()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nindex2tag[-100] = \"IGN\"\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(\n    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf[\"labels\"] = df[\"labels\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf['loss'] = df.apply(\n    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\ndf['predicted_label'] = df.apply(\n    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\ndf.head(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndf_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\ndf_tokens.head(7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n    .sort_values(by=\"sum\", ascending=False)\n    .reset_index()\n    .round(2)\n    .head(10)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    df_tokens.groupby(\"labels\")[[\"loss\"]] \n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)\n    .sort_values(by=\"mean\", ascending=False)\n    .reset_index()\n    .round(2)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n                      tags.names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndef get_samples(df):\n    for _, row in df.iterrows():\n        labels, preds, tokens, losses = [], [], [], []\n        for i, mask in enumerate(row[\"attention_mask\"]):\n            if i not in {0, len(row[\"attention_mask\"])}:\n                labels.append(row[\"labels\"][i])\n                preds.append(row[\"predicted_label\"][i])\n                tokens.append(row[\"input_tokens\"][i])\n                losses.append(f\"{row['loss'][i]:.2f}\")\n        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n                               \"preds\": preds, \"losses\": losses}).T\n        yield df_tmp\n\ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndf_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Lingual Transfer","metadata":{}},{"cell_type":"code","source":"def get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics[\"test_f1\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\ntag_text(text_fr, tags, trainer.model, xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx_ch[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nf1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nf1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nf1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### When Does Zero-Shot Transfer Make Sense?","metadata":{}},{"cell_type":"code","source":"def train_on_subset(dataset, num_samples):\n    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n    valid_ds = dataset[\"validation\"]\n    test_ds = dataset[\"test\"]\n    training_args.logging_steps = len(train_ds) // batch_size\n    \n    trainer = Trainer(model_init=model_init, args=training_args,\n        data_collator=data_collator, compute_metrics=compute_metrics,\n        train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n    trainer.train()\n    if training_args.push_to_hub:\n        trainer.push_to_hub(commit_message=\"Training completed!\")\n    \n    f1_score = get_f1_score(trainer, test_ds)\n    return pd.DataFrame.from_dict(\n        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.push_to_hub = False\nmetrics_df = train_on_subset(panx_fr_encoded, 250)\nmetrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\n# Hack needed to exclude the progress bars in the above cell\nmetrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfor num_samples in [500, 1000, 2000, 4000]:\n    metrics_df = metrics_df.append(\n        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\nplt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\nplt.ylim((0, 1))\nplt.xlabel(\"Number of Training Samples\")\nplt.ylabel(\"F1 Score\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning on Multiple Languages at Once","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ndef concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets(\n            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\ntraining_args.push_to_hub = True\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n    eval_dataset=panx_de_fr_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ncorpora = [panx_de_encoded]\n\n# Exclude German from iteration\nfor lang in langs[1:]:\n    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n    # Fine-tune on monolingual corpus\n    ds_encoded = encode_panx_dataset(panx_ch[lang])\n    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n    # Collect F1-scores in common dict\n    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n    # Add monolingual corpus to list of corpora to concatenate\n    corpora.append(ds_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpora_encoded = concatenate_splits(corpora)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n    eval_dataset=corpora_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfor idx, lang in enumerate(langs):\n    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_data = {\"de\": f1_scores[\"de\"],\n               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n               \"all\": f1_scores[\"all\"]}\nf1_scores_df = pd.DataFrame(scores_data).T.round(4)\nf1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\n                         inplace=True)\nf1_scores_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interacting with Model Widgets","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"A Hub widget\" caption=\"Example of a widget on the Hugging Face Hub\" src=\"images/chapter04_ner-widget.png\" id=\"ner-widget\"/>  ","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}