{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n!git clone https://github.com/nlp-with-transformers/notebooks.git\n%cd notebooks\nfrom install import *\ninstall_requirements()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:25:16.316988Z","iopub.execute_input":"2023-03-01T15:25:16.317505Z","iopub.status.idle":"2023-03-01T15:25:37.656240Z","shell.execute_reply.started":"2023-03-01T15:25:16.317461Z","shell.execute_reply":"2023-03-01T15:25:37.654341Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Cloning into 'notebooks'...\nremote: Enumerating objects: 515, done.\u001b[K\nremote: Counting objects: 100% (515/515), done.\u001b[K\nremote: Compressing objects: 100% (278/278), done.\u001b[K\nremote: Total 515 (delta 245), reused 479 (delta 231), pack-reused 0\u001b[K\nReceiving objects: 100% (515/515), 29.39 MiB | 19.13 MiB/s, done.\nResolving deltas: 100% (245/245), done.\n/kaggle/working/notebooks/notebooks/notebooks\nâ³ Installing base requirements ...\nâœ… Base requirements installed!\nâ³ Installing Git LFS ...\nâœ… Git LFS installed!\n","output_type":"stream"}]},{"cell_type":"code","source":"#hide\nfrom utils import *\nsetup_chapter()","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:25:37.659256Z","iopub.execute_input":"2023-03-01T15:25:37.660339Z","iopub.status.idle":"2023-03-01T15:25:37.676205Z","shell.execute_reply.started":"2023-03-01T15:25:37.660253Z","shell.execute_reply":"2023-03-01T15:25:37.674641Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"No GPU was detected! This notebook can be *very* slow without a GPU ðŸ¢\nGo to Settings > Accelerator and select GPU.\nUsing transformers v4.11.3\nUsing datasets v1.16.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multilingual Named Entity Recognition","metadata":{}},{"cell_type":"markdown","source":"## The Dataset","metadata":{}},{"cell_type":"code","source":"#id jeff-dean-ner\n#caption An example of a sequence annotated with named entities\n#hide_input\nimport pandas as pd\ntoks = \"Jeff Dean is a computer scientist at Google in California\".split()\nlbls = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\ndf = pd.DataFrame(data=[toks, lbls], index=['Tokens', 'Tags'])\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:25:37.678264Z","iopub.execute_input":"2023-03-01T15:25:37.679071Z","iopub.status.idle":"2023-03-01T15:25:37.715179Z","shell.execute_reply.started":"2023-03-01T15:25:37.679011Z","shell.execute_reply":"2023-03-01T15:25:37.712769Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"            0      1   2  3         4          5   6       7   8           9\nTokens   Jeff   Dean  is  a  computer  scientist  at  Google  in  California\nTags    B-PER  I-PER   O  O         O          O   O   B-ORG   O       B-LOC","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>Jeff</td>\n      <td>Dean</td>\n      <td>is</td>\n      <td>a</td>\n      <td>computer</td>\n      <td>scientist</td>\n      <td>at</td>\n      <td>Google</td>\n      <td>in</td>\n      <td>California</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>B-PER</td>\n      <td>I-PER</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-ORG</td>\n      <td>O</td>\n      <td>B-LOC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* To load one of the `PAN-X` subsets in `XTREME`, we'll nee to know which data configuration to pass the `load_dataset()` function. Let's use the `get_dataset_config_names()` function to find out which subsets are available:","metadata":{}},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:25:37.718730Z","iopub.execute_input":"2023-03-01T15:25:37.720281Z","iopub.status.idle":"2023-03-01T15:25:38.664353Z","shell.execute_reply.started":"2023-03-01T15:25:37.720213Z","shell.execute_reply":"2023-03-01T15:25:38.662083Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"XTREME has 183 configurations\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* A lot of configuration! Let's narrow our search:","metadata":{}},{"cell_type":"code","source":"# finda configurations starts with PAN\npanx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\npanx_subsets[:3]","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:25:38.666657Z","iopub.execute_input":"2023-03-01T15:25:38.667246Z","iopub.status.idle":"2023-03-01T15:25:38.679669Z","shell.execute_reply.started":"2023-03-01T15:25:38.667171Z","shell.execute_reply":"2023-03-01T15:25:38.678062Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"},"metadata":{}}]},{"cell_type":"code","source":"# load the German corpus\nfrom datasets import load_dataset\n\nload_dataset(\"xtreme\", name=\"PAN-X.de\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:25:38.681680Z","iopub.execute_input":"2023-03-01T15:25:38.682128Z","iopub.status.idle":"2023-03-01T15:25:39.390590Z","shell.execute_reply.started":"2023-03-01T15:25:38.682089Z","shell.execute_reply":"2023-03-01T15:25:39.388687Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"110d37922095478793582b4ccd96316a"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    train: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"* Lets create a realistic Swiss corpus. That is, we'll sample German, French, Italian, and English corpora from `PAN-X` according to their spoken proportions:","metadata":{}},{"cell_type":"code","source":"# hide_output\nfrom collections import defaultdict # same as dictionary but it never raises a KeyError\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n# Return a DatasetDict if a key doesn't exist\npanx_ch = defaultdict(DatasetDict) # Multilingual corpus\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    mono_corpus = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    \n    # Shuffle and downsample each split according to spoken proportion\n    for split in mono_corpus:               # train, test, validation\n        panx_ch[lang][split] = (\n            mono_corpus[split]\n            # avoid bias in dataset spolits\n            .shuffle(seed=0)\n            # downsample each corpus according to the values in fracs\n            .select(range(int(frac * mono_corpus[split].num_rows))))","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:34:37.153849Z","iopub.execute_input":"2023-03-01T15:34:37.154269Z","iopub.status.idle":"2023-03-01T15:34:39.944247Z","shell.execute_reply.started":"2023-03-01T15:34:37.154233Z","shell.execute_reply":"2023-03-01T15:34:39.940880Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2236edd899344a74b03fa808a44ee187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3eb22f1737c4c51b099af1263219c13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbf50e1113aa4b0086fcd96c6797e950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78119112a07b40c387f370727e219448"}},"metadata":{}}]},{"cell_type":"code","source":"{lang: panx_ch[lang][\"train\"].num_rows for lang in langs}","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:38:41.295191Z","iopub.execute_input":"2023-03-01T15:38:41.295863Z","iopub.status.idle":"2023-03-01T15:38:41.310359Z","shell.execute_reply.started":"2023-03-01T15:38:41.295805Z","shell.execute_reply":"2023-03-01T15:38:41.308735Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'de': 12580, 'fr': 4580, 'it': 1680, 'en': 1180}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n# number of example per language in the training split in the multilingual corpus\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n             index=[\"Number of training examples\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:38:59.265526Z","iopub.execute_input":"2023-03-01T15:38:59.265995Z","iopub.status.idle":"2023-03-01T15:38:59.282259Z","shell.execute_reply.started":"2023-03-01T15:38:59.265958Z","shell.execute_reply":"2023-03-01T15:38:59.280697Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                de    fr    it    en\nNumber of training examples  12580  4580  1680  1180","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de</th>\n      <th>fr</th>\n      <th>it</th>\n      <th>en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of training examples</th>\n      <td>12580</td>\n      <td>4580</td>\n      <td>1680</td>\n      <td>1180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"element = panx_ch[\"de\"][\"train\"][0]\nfor key, value in element.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:43:32.971193Z","iopub.execute_input":"2023-03-01T15:43:32.971681Z","iopub.status.idle":"2023-03-01T15:43:32.979766Z","shell.execute_reply.started":"2023-03-01T15:43:32.971643Z","shell.execute_reply":"2023-03-01T15:43:32.978525Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der',\n'polnischen', 'Woiwodschaft', 'Pommern', '.']\nner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\nlangs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n","output_type":"stream"}]},{"cell_type":"code","source":"for key, value in panx_ch[\"de\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:54:54.287122Z","iopub.execute_input":"2023-03-01T15:54:54.287600Z","iopub.status.idle":"2023-03-01T15:54:54.294427Z","shell.execute_reply.started":"2023-03-01T15:54:54.287560Z","shell.execute_reply":"2023-03-01T15:54:54.293430Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\nner_tags: Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER',\n'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None),\nlength=-1, id=None)\nlangs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T15:55:28.508523Z","iopub.execute_input":"2023-03-01T15:55:28.508915Z","iopub.status.idle":"2023-03-01T15:55:28.517167Z","shell.execute_reply.started":"2023-03-01T15:55:28.508882Z","shell.execute_reply":"2023-03-01T15:55:28.515346Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n'B-LOC', 'I-LOC'], names_file=None, id=None)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* We can use `ClassLabel.int2str()` method to create a new column in our training set with class names for each tag. We'll use the `map()` method to return a `dict` with keys corresponding to the new column name and the value as a `list` of class names:","metadata":{}},{"cell_type":"code","source":"# hide_output\ndef create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:08:41.483085Z","iopub.execute_input":"2023-03-01T16:08:41.484280Z","iopub.status.idle":"2023-03-01T16:08:45.881583Z","shell.execute_reply.started":"2023-03-01T16:08:41.484208Z","shell.execute_reply":"2023-03-01T16:08:45.880231Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df2098f6b4c846b49d037f53f7b0544c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de87b85507f4a5d8fb7340be5c06f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12580 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2984299699241708d7fc654928483b5"}},"metadata":{}}]},{"cell_type":"code","source":"# hide_output\nde_example = panx_de[\"train\"][0]\npd.DataFrame(data=[de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\nindex=['Tokens', 'Tags'])","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:11:55.550180Z","iopub.execute_input":"2023-03-01T16:11:55.550615Z","iopub.status.idle":"2023-03-01T16:11:55.570638Z","shell.execute_reply.started":"2023-03-01T16:11:55.550560Z","shell.execute_reply":"2023-03-01T16:11:55.569356Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"           0           1   2    3         4      5   6    7           8   \\\nTokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \nTags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n\n                  9        10 11  \nTokens  Woiwodschaft  Pommern  .  \nTags           B-LOC    I-LOC  O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>2.000</td>\n      <td>Einwohnern</td>\n      <td>an</td>\n      <td>der</td>\n      <td>Danziger</td>\n      <td>Bucht</td>\n      <td>in</td>\n      <td>der</td>\n      <td>polnischen</td>\n      <td>Woiwodschaft</td>\n      <td>Pommern</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Let's calculate the frequencies of each entity across each split:","metadata":{}},{"cell_type":"code","source":"# import Counter class from collections module\nfrom collections import Counter   \n\n# create defaultdict to store frequency of each NER tag type for each split\nsplit2freqs = defaultdict(Counter)\n\n# iterate over splits in the mulitlingual corpus\nfor split, dataset in panx_de.items():\n    # iterate over rows in current split\n    for row in dataset[\"ner_tags_str\"]:\n         # iterate over NER tags in current row\n        for tag in row:                            \n            if tag.startswith(\"B\"):                   # if tag is a beginning tag\n                tag_type = tag.split(\"-\")[1]          # extract tag type\n                split2freqs[split][tag_type] += 1     # increment count of tag type in current split\n                \n# create DataFrame from split2freqs defaultdict and display it\npd.DataFrame.from_dict(split2freqs, orient=\"index\")  ","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:22:06.655533Z","iopub.execute_input":"2023-03-01T16:22:06.655962Z","iopub.status.idle":"2023-03-01T16:22:07.026187Z","shell.execute_reply.started":"2023-03-01T16:22:06.655924Z","shell.execute_reply":"2023-03-01T16:22:07.025277Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"             ORG   LOC   PER\nvalidation  2683  3172  2893\ntest        2573  3180  3071\ntrain       5366  6186  5810","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ORG</th>\n      <th>LOC</th>\n      <th>PER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>validation</th>\n      <td>2683</td>\n      <td>3172</td>\n      <td>2893</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>2573</td>\n      <td>3180</td>\n      <td>3071</td>\n    </tr>\n    <tr>\n      <th>train</th>\n      <td>5366</td>\n      <td>6186</td>\n      <td>5810</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Multilingual Transformers","metadata":{}},{"cell_type":"markdown","source":"## A Closer Look at Tokenization","metadata":{}},{"cell_type":"code","source":"# hide_output\nfrom transformers import AutoTokenizer\n\nbert_model_name = \"bert-base-cased\"\nxlmr_model_name = \"xlm-roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"Jack Sparrow loves New York!\"\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\ndf = pd.DataFrame([bert_tokens, xlmr_tokens], index=[\"BERT\", \"XLM-R\"])\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Tokenizer Pipeline","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Tokenizer pipeline\" caption=\"The steps in the tokenization pipeline\" src=\"images/chapter04_tokenizer-pipeline.png\" id=\"toknizer-pipeline\"/>","metadata":{}},{"cell_type":"markdown","source":"### The SentencePiece Tokenizer","metadata":{}},{"cell_type":"code","source":"\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"[[train_ner_tagger]]","metadata":{}},{"cell_type":"markdown","source":"## Transformers for Named Entity Recognition","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Architecture of a transformer encoder for classification.\" caption=\"Fine-tuning an encoder-based transformer for sequence classification\" src=\"images/chapter04_clf-architecture.png\" id=\"clf-arch\"/>","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Architecture of a transformer encoder for named entity recognition. The wide linear layer shows that the same linear layer is applied to all hidden states.\" caption=\"Fine-tuning an encoder-based transformer for named entity recognition\" src=\"images/chapter04_ner-architecture.png\" id=\"ner-arch\"/>","metadata":{}},{"cell_type":"markdown","source":"## The Anatomy of the Transformers Model Class","metadata":{}},{"cell_type":"markdown","source":"### Bodies and Heads","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"bert-body-head\" caption=\"The `BertModel` class only contains the body of the model, while the `BertFor&lt;Task&gt;` classes combine the body with a dedicated head for a given task\" src=\"images/chapter04_bert-body-head.png\" id=\"bert-body-head\"/>","metadata":{}},{"cell_type":"markdown","source":"### Creating a Custom Model for Token Classification","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    config_class = XLMRobertaConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        # Load model body\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        # Set up token classification head\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Load and initialize weights\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                labels=None, **kwargs):\n        # Use model body to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n                               token_type_ids=token_type_ids, **kwargs)\n        # Apply classifier to encoder representation\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        # Calculate losses\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        # Return model output object\n        return TokenClassifierOutput(loss=loss, logits=logits, \n                                     hidden_states=outputs.hidden_states, \n                                     attentions=outputs.attentions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading a Custom Model","metadata":{}},{"cell_type":"code","source":"index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfrom transformers import AutoConfig\n\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name, \n                                         num_labels=tags.num_classes,\n                                         id2label=index2tag, label2id=tag2index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlmr_model = (XLMRobertaForTokenClassification\n              .from_pretrained(xlmr_model_name, config=xlmr_config)\n              .to(device))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = xlmr_model(input_ids.to(device)).logits\npredictions = torch.argmax(outputs, dim=-1)\nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\nprint(f\"Shape of outputs: {outputs.shape}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tag_text(text, tags, model, tokenizer):\n    # Get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # Encode the sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # Get predictions as distribution over 7 possible classes\n    outputs = model(input_ids)[0]\n    # Take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=2)\n    # Convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing Texts for NER","metadata":{}},{"cell_type":"code","source":"words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\npd.DataFrame([tokens], index=[\"Tokens\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nword_ids = tokenized_input.word_ids()\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nprevious_word_idx = None\nlabel_ids = []\n\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx])\n    previous_word_idx = word_idx\n    \nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n                                      is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, batched=True, \n                      remove_columns=['langs', 'ner_tags', 'tokens'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Measures","metadata":{}},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\nprint(classification_report(y_true, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n\n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n\n    return preds_list, labels_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning XLM-RoBERTa","metadata":{}},{"cell_type":"code","source":"# hide_output\nfrom transformers import TrainingArguments\n\nnum_epochs = 3\nbatch_size = 24\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\ntraining_args = TrainingArguments(\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n    logging_steps=logging_steps, push_to_hub=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, \n                                       eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_init():\n    return (XLMRobertaForTokenClassification\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\n            .to(device))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide\n%env TOKENIZERS_PARALLELISM=false","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfrom transformers import Trainer\n\ntrainer = Trainer(model_init=model_init, args=training_args, \n                  data_collator=data_collator, compute_metrics=compute_metrics,\n                  train_dataset=panx_de_encoded[\"train\"],\n                  eval_dataset=panx_de_encoded[\"validation\"], \n                  tokenizer=xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\ndf = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss', 'eval_f1']]\ndf = df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", \"eval_f1\":\"F1\"})\ndf['Epoch'] = df[\"Epoch\"].apply(lambda x: round(x))\ndf['Training Loss'] = df[\"Training Loss\"].ffill()\ndf[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\ndf.drop_duplicates()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error Analysis","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # Convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    # Pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        # Pass data through model  \n        output = trainer.model(input_ids, attention_mask)\n        # Logit.size: [batch_size, sequence_length, classes]\n        # Predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    # Calculate loss per token after flattening batch dimension with view\n    loss = cross_entropy(output.logits.view(-1, 7), \n                         labels.view(-1), reduction=\"none\")\n    # Unflatten batch dimension and convert to numpy array\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n\n    return {\"loss\":loss, \"predicted_label\": predicted_label}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nvalid_set = panx_de_encoded[\"validation\"]\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = valid_set.to_pandas()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nindex2tag[-100] = \"IGN\"\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(\n    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf[\"labels\"] = df[\"labels\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf['loss'] = df.apply(\n    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\ndf['predicted_label'] = df.apply(\n    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\ndf.head(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndf_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\ndf_tokens.head(7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n    .sort_values(by=\"sum\", ascending=False)\n    .reset_index()\n    .round(2)\n    .head(10)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    df_tokens.groupby(\"labels\")[[\"loss\"]] \n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)\n    .sort_values(by=\"mean\", ascending=False)\n    .reset_index()\n    .round(2)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n                      tags.names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndef get_samples(df):\n    for _, row in df.iterrows():\n        labels, preds, tokens, losses = [], [], [], []\n        for i, mask in enumerate(row[\"attention_mask\"]):\n            if i not in {0, len(row[\"attention_mask\"])}:\n                labels.append(row[\"labels\"][i])\n                preds.append(row[\"predicted_label\"][i])\n                tokens.append(row[\"input_tokens\"][i])\n                losses.append(f\"{row['loss'][i]:.2f}\")\n        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n                               \"preds\": preds, \"losses\": losses}).T\n        yield df_tmp\n\ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndf_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Lingual Transfer","metadata":{}},{"cell_type":"code","source":"def get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics[\"test_f1\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\ntag_text(text_fr, tags, trainer.model, xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx_ch[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nf1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nf1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nf1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### When Does Zero-Shot Transfer Make Sense?","metadata":{}},{"cell_type":"code","source":"def train_on_subset(dataset, num_samples):\n    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n    valid_ds = dataset[\"validation\"]\n    test_ds = dataset[\"test\"]\n    training_args.logging_steps = len(train_ds) // batch_size\n    \n    trainer = Trainer(model_init=model_init, args=training_args,\n        data_collator=data_collator, compute_metrics=compute_metrics,\n        train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n    trainer.train()\n    if training_args.push_to_hub:\n        trainer.push_to_hub(commit_message=\"Training completed!\")\n    \n    f1_score = get_f1_score(trainer, test_ds)\n    return pd.DataFrame.from_dict(\n        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.push_to_hub = False\nmetrics_df = train_on_subset(panx_fr_encoded, 250)\nmetrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\n# Hack needed to exclude the progress bars in the above cell\nmetrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfor num_samples in [500, 1000, 2000, 4000]:\n    metrics_df = metrics_df.append(\n        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\nplt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\nplt.ylim((0, 1))\nplt.xlabel(\"Number of Training Samples\")\nplt.ylabel(\"F1 Score\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning on Multiple Languages at Once","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ndef concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets(\n            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\ntraining_args.push_to_hub = True\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n    eval_dataset=panx_de_fr_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ncorpora = [panx_de_encoded]\n\n# Exclude German from iteration\nfor lang in langs[1:]:\n    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n    # Fine-tune on monolingual corpus\n    ds_encoded = encode_panx_dataset(panx_ch[lang])\n    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n    # Collect F1-scores in common dict\n    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n    # Add monolingual corpus to list of corpora to concatenate\n    corpora.append(ds_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpora_encoded = concatenate_splits(corpora)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n    eval_dataset=corpora_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfor idx, lang in enumerate(langs):\n    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_data = {\"de\": f1_scores[\"de\"],\n               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n               \"all\": f1_scores[\"all\"]}\nf1_scores_df = pd.DataFrame(scores_data).T.round(4)\nf1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\n                         inplace=True)\nf1_scores_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interacting with Model Widgets","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"A Hub widget\" caption=\"Example of a widget on the Hugging Face Hub\" src=\"images/chapter04_ner-widget.png\" id=\"ner-widget\"/>  ","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}