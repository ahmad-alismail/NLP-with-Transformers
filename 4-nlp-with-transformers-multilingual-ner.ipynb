{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n!git clone https://github.com/nlp-with-transformers/notebooks.git\n%cd notebooks\nfrom install import *\ninstall_requirements()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:51:50.327257Z","iopub.execute_input":"2023-03-02T08:51:50.327680Z","iopub.status.idle":"2023-03-02T08:53:11.429109Z","shell.execute_reply.started":"2023-03-02T08:51:50.327643Z","shell.execute_reply":"2023-03-02T08:53:11.427654Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'notebooks'...\nremote: Enumerating objects: 515, done.\u001b[K\nremote: Counting objects: 100% (515/515), done.\u001b[K\nremote: Compressing objects: 100% (278/278), done.\u001b[K\nremote: Total 515 (delta 245), reused 479 (delta 231), pack-reused 0\u001b[K\nReceiving objects: 100% (515/515), 29.39 MiB | 17.06 MiB/s, done.\nResolving deltas: 100% (245/245), done.\n/kaggle/working/notebooks\n⏳ Installing base requirements ...\n✅ Base requirements installed!\n⏳ Installing Git LFS ...\n✅ Git LFS installed!\n","output_type":"stream"}]},{"cell_type":"code","source":"#hide\nfrom utils import *\nsetup_chapter()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:53:11.431801Z","iopub.execute_input":"2023-03-02T08:53:11.432383Z","iopub.status.idle":"2023-03-02T08:53:14.743667Z","shell.execute_reply.started":"2023-03-02T08:53:11.432342Z","shell.execute_reply":"2023-03-02T08:53:14.742542Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"No GPU was detected! This notebook can be *very* slow without a GPU 🐢\nGo to Settings > Accelerator and select GPU.\nUsing transformers v4.11.3\nUsing datasets v1.16.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multilingual Named Entity Recognition","metadata":{}},{"cell_type":"markdown","source":"## The Dataset","metadata":{}},{"cell_type":"code","source":"#id jeff-dean-ner\n#caption An example of a sequence annotated with named entities\n#hide_input\nimport pandas as pd\ntoks = \"Jeff Dean is a computer scientist at Google in California\".split()\nlbls = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\ndf = pd.DataFrame(data=[toks, lbls], index=['Tokens', 'Tags'])\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:53:14.744960Z","iopub.execute_input":"2023-03-02T08:53:14.745992Z","iopub.status.idle":"2023-03-02T08:53:14.782146Z","shell.execute_reply.started":"2023-03-02T08:53:14.745952Z","shell.execute_reply":"2023-03-02T08:53:14.780850Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            0      1   2  3         4          5   6       7   8           9\nTokens   Jeff   Dean  is  a  computer  scientist  at  Google  in  California\nTags    B-PER  I-PER   O  O         O          O   O   B-ORG   O       B-LOC","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>Jeff</td>\n      <td>Dean</td>\n      <td>is</td>\n      <td>a</td>\n      <td>computer</td>\n      <td>scientist</td>\n      <td>at</td>\n      <td>Google</td>\n      <td>in</td>\n      <td>California</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>B-PER</td>\n      <td>I-PER</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-ORG</td>\n      <td>O</td>\n      <td>B-LOC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* To load one of the `PAN-X` subsets in `XTREME`, we'll nee to know which data configuration to pass the `load_dataset()` function. Let's use the `get_dataset_config_names()` function to find out which subsets are available:","metadata":{}},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:53:14.784665Z","iopub.execute_input":"2023-03-02T08:53:14.785034Z","iopub.status.idle":"2023-03-02T08:53:16.309661Z","shell.execute_reply.started":"2023-03-02T08:53:14.784998Z","shell.execute_reply":"2023-03-02T08:53:16.308421Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/9.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa7a7f60b046429692e99d621f3b72f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/23.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ad8e9ccb4242389e443bfc78a8f16c"}},"metadata":{}},{"name":"stdout","text":"XTREME has 183 configurations\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* A lot of configuration! Let's narrow our search:","metadata":{}},{"cell_type":"code","source":"# finda configurations starts with PAN\npanx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\npanx_subsets[:3]","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:53:16.310917Z","iopub.execute_input":"2023-03-02T08:53:16.311766Z","iopub.status.idle":"2023-03-02T08:53:16.321836Z","shell.execute_reply.started":"2023-03-02T08:53:16.311728Z","shell.execute_reply":"2023-03-02T08:53:16.320630Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"},"metadata":{}}]},{"cell_type":"code","source":"# load the German corpus\nfrom datasets import load_dataset\n\nload_dataset(\"xtreme\", name=\"PAN-X.de\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:53:16.323106Z","iopub.execute_input":"2023-03-02T08:53:16.323450Z","iopub.status.idle":"2023-03-02T08:53:35.755655Z","shell.execute_reply.started":"2023-03-02T08:53:16.323417Z","shell.execute_reply":"2023-03-02T08:53:35.754315Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.de (download: 223.17 MiB, generated: 9.08 MiB, post-processed: Unknown size, total: 232.25 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/234M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e166c590654747ed896e653788210136"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac21647d36c8423fa25bec7003734df0"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    train: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 20000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"* Lets create a realistic Swiss corpus. That is, we'll sample German, French, Italian, and English corpora from `PAN-X` according to their spoken proportions:","metadata":{}},{"cell_type":"code","source":"# hide_output\nfrom collections import defaultdict # same as dictionary but it never raises a KeyError\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n# Return a DatasetDict if a key doesn't exist\npanx_ch = defaultdict(DatasetDict) # Multilingual corpus\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    mono_corpus = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    \n    # Shuffle and downsample each split according to spoken proportion\n    for split in mono_corpus:               # train, test, validation\n        panx_ch[lang][split] = (\n            mono_corpus[split]\n            # avoid bias in dataset spolits\n            .shuffle(seed=0)\n            # downsample each corpus according to the values in fracs\n            .select(range(int(frac * mono_corpus[split].num_rows))))","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:53:35.756907Z","iopub.execute_input":"2023-03-02T08:53:35.757269Z","iopub.status.idle":"2023-03-02T08:54:01.030277Z","shell.execute_reply.started":"2023-03-02T08:53:35.757233Z","shell.execute_reply":"2023-03-02T08:54:01.029063Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e19e38d99044f18a69f9f2207135072"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.fr (download: 223.17 MiB, generated: 6.37 MiB, post-processed: Unknown size, total: 229.53 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"783ee07936b04c7a9ec6ae8f34e1d78d"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.it (download: 223.17 MiB, generated: 7.35 MiB, post-processed: Unknown size, total: 230.52 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15ebe01c6f514f418e1f1cd2d43c3dcc"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.en (download: 223.17 MiB, generated: 7.30 MiB, post-processed: Unknown size, total: 230.47 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/2fc6b63c5326cc0d1f73060649612889b3a7ed8a6605c91cecdbd228a7158b17. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fafdb26cdd843be94238c375a358233"}},"metadata":{}}]},{"cell_type":"code","source":"{lang: panx_ch[lang][\"train\"].num_rows for lang in langs}","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:54:01.031854Z","iopub.execute_input":"2023-03-02T08:54:01.032340Z","iopub.status.idle":"2023-03-02T08:54:01.040145Z","shell.execute_reply.started":"2023-03-02T08:54:01.032299Z","shell.execute_reply":"2023-03-02T08:54:01.038993Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'de': 12580, 'fr': 4580, 'it': 1680, 'en': 1180}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n# number of example per language in the training split in the multilingual corpus\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n             index=[\"Number of training examples\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:54:01.041564Z","iopub.execute_input":"2023-03-02T08:54:01.042225Z","iopub.status.idle":"2023-03-02T08:54:01.942789Z","shell.execute_reply.started":"2023-03-02T08:54:01.042187Z","shell.execute_reply":"2023-03-02T08:54:01.941612Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                de    fr    it    en\nNumber of training examples  12580  4580  1680  1180","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de</th>\n      <th>fr</th>\n      <th>it</th>\n      <th>en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of training examples</th>\n      <td>12580</td>\n      <td>4580</td>\n      <td>1680</td>\n      <td>1180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"element = panx_ch[\"de\"][\"train\"][0]\nfor key, value in element.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:54:01.947309Z","iopub.execute_input":"2023-03-02T08:54:01.947702Z","iopub.status.idle":"2023-03-02T08:54:01.955333Z","shell.execute_reply.started":"2023-03-02T08:54:01.947665Z","shell.execute_reply":"2023-03-02T08:54:01.954058Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der',\n'polnischen', 'Woiwodschaft', 'Pommern', '.']\nner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\nlangs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n","output_type":"stream"}]},{"cell_type":"code","source":"for key, value in panx_ch[\"de\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:54:01.956601Z","iopub.execute_input":"2023-03-02T08:54:01.956915Z","iopub.status.idle":"2023-03-02T08:54:01.966478Z","shell.execute_reply.started":"2023-03-02T08:54:01.956885Z","shell.execute_reply":"2023-03-02T08:54:01.965173Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\nner_tags: Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER',\n'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None),\nlength=-1, id=None)\nlangs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:54:01.967545Z","iopub.execute_input":"2023-03-02T08:54:01.967984Z","iopub.status.idle":"2023-03-02T08:54:01.976382Z","shell.execute_reply.started":"2023-03-02T08:54:01.967935Z","shell.execute_reply":"2023-03-02T08:54:01.975174Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n'B-LOC', 'I-LOC'], names_file=None, id=None)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* We can use `ClassLabel.int2str()` method to create a new column in our training set with class names for each tag. We'll use the `map()` method to return a `dict` with keys corresponding to the new column name and the value as a `list` of class names:","metadata":{}},{"cell_type":"code","source":"# hide_output\ndef create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:54:01.977832Z","iopub.execute_input":"2023-03-02T08:54:01.978177Z","iopub.status.idle":"2023-03-02T08:54:06.177146Z","shell.execute_reply.started":"2023-03-02T08:54:01.978110Z","shell.execute_reply":"2023-03-02T08:54:06.175921Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e180c89ea24da8ba1d1e6b24ab30d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37ae163f66c4420aba590ffbc364d520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12580 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf22b3add9be41b5a3b6535555197ed5"}},"metadata":{}}]},{"cell_type":"code","source":"# hide_output\nde_example = panx_de[\"train\"][0]\npd.DataFrame(data=[de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\nindex=['Tokens', 'Tags'])","metadata":{"execution":{"iopub.status.busy":"2023-03-02T08:54:06.178949Z","iopub.execute_input":"2023-03-02T08:54:06.179317Z","iopub.status.idle":"2023-03-02T08:54:06.197253Z","shell.execute_reply.started":"2023-03-02T08:54:06.179282Z","shell.execute_reply":"2023-03-02T08:54:06.196214Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"           0           1   2    3         4      5   6    7           8   \\\nTokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \nTags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n\n                  9        10 11  \nTokens  Woiwodschaft  Pommern  .  \nTags           B-LOC    I-LOC  O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>2.000</td>\n      <td>Einwohnern</td>\n      <td>an</td>\n      <td>der</td>\n      <td>Danziger</td>\n      <td>Bucht</td>\n      <td>in</td>\n      <td>der</td>\n      <td>polnischen</td>\n      <td>Woiwodschaft</td>\n      <td>Pommern</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Let's calculate the frequencies of each entity across each split:","metadata":{}},{"cell_type":"markdown","source":"````python\npanx_de.items()\n> dict_items([('validation', Dataset({\n    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n    num_rows: 6290\n})), ('test', Dataset({\n    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n    num_rows: 6290\n})), ('train', Dataset({\n    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n    num_rows: 12580\n}))])\n````","metadata":{}},{"cell_type":"code","source":"# import Counter class from collections module\nfrom collections import Counter   \n\n# create defaultdict to store frequency of each NER tag type for each split\nsplit2freqs = defaultdict(Counter)\n\n# iterate over splits in the mulitlingual corpus\nfor split, dataset in panx_de.items():\n    # iterate over rows in current split\n    for row in dataset[\"ner_tags_str\"]:\n         # iterate over NER tags in current row\n        for tag in row:                            \n            if tag.startswith(\"B\"):                   # if tag is a beginning tag\n                tag_type = tag.split(\"-\")[1]          # extract tag type\n                split2freqs[split][tag_type] += 1     # increment count of tag type in current split\n                \n# create DataFrame from split2freqs defaultdict and display it\npd.DataFrame.from_dict(split2freqs, orient=\"index\")  ","metadata":{"execution":{"iopub.status.busy":"2023-03-01T16:22:06.655533Z","iopub.execute_input":"2023-03-01T16:22:06.655962Z","iopub.status.idle":"2023-03-01T16:22:07.026187Z","shell.execute_reply.started":"2023-03-01T16:22:06.655924Z","shell.execute_reply":"2023-03-01T16:22:07.025277Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"             ORG   LOC   PER\nvalidation  2683  3172  2893\ntest        2573  3180  3071\ntrain       5366  6186  5810","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ORG</th>\n      <th>LOC</th>\n      <th>PER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>validation</th>\n      <td>2683</td>\n      <td>3172</td>\n      <td>2893</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>2573</td>\n      <td>3180</td>\n      <td>3071</td>\n    </tr>\n    <tr>\n      <th>train</th>\n      <td>5366</td>\n      <td>6186</td>\n      <td>5810</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Multilingual Transformers","metadata":{}},{"cell_type":"markdown","source":"* Multilingual transformers invole similar architecture and training proceduers as their monolingual counterparts, except using multilingual corpus for pre training.\n* They can generalize well across languages for variety of downstream tasks.\n* Multilingual transformers are usually evaluated in three different ways:\n    * `en`: **Fine-tune** on the English training data and then **evaluate** on each language's test set.\n    * `each`: **Fine-tune** and **evaluate** on monolingual test data to measure per-language performance.\n    * `all`: **Fine-tune** on all the training data to **evaluate** on all on each language's test set.\n","metadata":{}},{"cell_type":"markdown","source":"## XLM-RoBERTa (aka, XLM-R)\n * We'll select XLM-RoBERTa as our multilingual transformer\n * RoBERTa part of XLM-RoBERTa means the the pretraining approach is the same as for monolingual RoBERTa that remove next sentence prediction task.\n * XLM-RoBERTa has a large vocabulary with 250000 tokens\n * XLM-RoBERTa uses `SentencePiece` instead of `WordPiece` to tokenize the raw text directly","metadata":{}},{"cell_type":"markdown","source":"## A Closer Look at Tokenization\nLet's compare BERT's `WordPiece` with XLM-RoBERTa's `SentencePiece` tokenizers:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbert_model_name = \"bert-base-cased\"\nxlmr_model_name = \"xlm-roberta-base\"\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)","metadata":{"execution":{"iopub.status.busy":"2023-03-02T09:38:12.535772Z","iopub.execute_input":"2023-03-02T09:38:12.536240Z","iopub.status.idle":"2023-03-02T09:38:40.172237Z","shell.execute_reply.started":"2023-03-02T09:38:12.536197Z","shell.execute_reply":"2023-03-02T09:38:40.171129Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1815c92eab4db5b4728c85db87fdc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4218b79e3fc84024a03c1c95200df3ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc443c499ef41aa9b93c97eebb9c9a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a9a97dfc8c423883053c12d547f9ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8945443e2fdd4068ae6892a8ade4ff0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee896fe4b55464d88fa5f262053dc3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3384aa7586ac495b9e3ea12ce85c38b5"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"Jack Sparrow loves New York!\"\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()","metadata":{"execution":{"iopub.status.busy":"2023-03-02T09:38:40.175496Z","iopub.execute_input":"2023-03-02T09:38:40.176399Z","iopub.status.idle":"2023-03-02T09:38:40.184776Z","shell.execute_reply.started":"2023-03-02T09:38:40.176338Z","shell.execute_reply":"2023-03-02T09:38:40.183364Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\ndf = pd.DataFrame(data=[bert_tokens, xlmr_tokens], index=[\"BERT\", \"XLM-R\"])\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-02T09:38:53.981057Z","iopub.execute_input":"2023-03-02T09:38:53.981493Z","iopub.status.idle":"2023-03-02T09:38:53.999678Z","shell.execute_reply.started":"2023-03-02T09:38:53.981455Z","shell.execute_reply":"2023-03-02T09:38:53.998512Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"           0      1      2       3      4    5     6      7      8     9\nBERT   [CLS]   Jack    Spa  ##rrow  loves  New  York      !  [SEP]  None\nXLM-R    <s>  ▁Jack  ▁Spar     row  ▁love    s  ▁New  ▁York      !  </s>","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BERT</th>\n      <td>[CLS]</td>\n      <td>Jack</td>\n      <td>Spa</td>\n      <td>##rrow</td>\n      <td>loves</td>\n      <td>New</td>\n      <td>York</td>\n      <td>!</td>\n      <td>[SEP]</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>XLM-R</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* First note: XLM-RoBERTa uses `<s>` and `<\\s>` instead of `[CLS]` and `[SEP]` to denote the start and end of a sequence\n\n\nLet's delve deeper to gain a better understanding of how the tokenizer works in general. Later, we will see what makes the `SentencePiece` tokenizer so special?\n","metadata":{}},{"cell_type":"markdown","source":"### The Tokenizer Pipeline","metadata":{}},{"cell_type":"markdown","source":"Tokenization is not just a single operation that transforms strings to integers we can pass through the model. Tokenization consists usually of four steps as shown below:","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Tokenizer pipeline\" caption=\"The steps in the tokenization pipeline\" src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter04_tokenizer-pipeline.png\" id=\"toknizer-pipeline\"/>","metadata":{}},{"cell_type":"markdown","source":"1. **Normalization**: A set of operations are applied to raw string to make it cleaner. For example, stripping whitespace, removing accented characters, unicode normalization, and lowercasing. For example, `\"jack sparrow loves new york!\"`\n2. **Pretokenization**: In this step we will split our text into smaller objects (i.e., words). For example `[\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]`. In the next step, these words are split into subwords with Byte-Pair Encoding (BPE) or Unigram algorithms.\n3. **Tokenizer model**: the tokenizer applies a **subword splitting model** (BPE, Unigram, WordPiece). This part of the pipleling needs to be trained on your corpus. \n> Note: The output of the subword splitting model is a **list of integers (Input IDs)**. For illustration, the output in strings would look like `[jack, spa,rrow, loves, new, york, !]`\n4. **Postprocessing**: this step add some transformation on the list of tokens, e.g., adding special tokens at the beginning or end of the input sequence of token indices. The output would be like `[[CLS], jack, spa, rrow, loves, new, york, !, [SEP]]`\n\nThe question that arises now is, what makes the `SentencePiece` tokenizer so special?","metadata":{}},{"cell_type":"markdown","source":"### The SentencePiece Tokenizer","metadata":{}},{"cell_type":"code","source":"\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"[[train_ner_tagger]]","metadata":{}},{"cell_type":"markdown","source":"## Transformers for Named Entity Recognition","metadata":{}},{"cell_type":"markdown","source":"* *Architecture of a transformer encoder for classification*","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Architecture of a transformer encoder for classification.\" caption=\"Fine-tuning an encoder-based transformer for sequence classification\" src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter04_clf-architecture.png\" id=\"clf-arch\"/>","metadata":{}},{"cell_type":"markdown","source":"* *Architecture of a transformer encoder for named entity recognition. The wide linear layer shows that the same linear layer is applied to all hidden states.*","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"Architecture of a transformer encoder for named entity recognition. The wide linear layer shows that the same linear layer is applied to all hidden states.\" caption=\"Fine-tuning an encoder-based transformer for named entity recognition\" src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter04_ner-architecture.png\" id=\"ner-arch\"/>","metadata":{}},{"cell_type":"markdown","source":"## The Anatomy of the Transformers Model Class","metadata":{}},{"cell_type":"markdown","source":"### Bodies and Heads","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"bert-body-head\" caption=\"The `BertModel` class only contains the body of the model, while the `BertFor&lt;Task&gt;` classes combine the body with a dedicated head for a given task\" src=\"images/chapter04_bert-body-head.png\" id=\"bert-body-head\"/>","metadata":{}},{"cell_type":"markdown","source":"### Creating a Custom Model for Token Classification","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    config_class = XLMRobertaConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        # Load model body\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        # Set up token classification head\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # Load and initialize weights\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                labels=None, **kwargs):\n        # Use model body to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n                               token_type_ids=token_type_ids, **kwargs)\n        # Apply classifier to encoder representation\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        # Calculate losses\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        # Return model output object\n        return TokenClassifierOutput(loss=loss, logits=logits, \n                                     hidden_states=outputs.hidden_states, \n                                     attentions=outputs.attentions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading a Custom Model","metadata":{}},{"cell_type":"code","source":"index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfrom transformers import AutoConfig\n\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name, \n                                         num_labels=tags.num_classes,\n                                         id2label=index2tag, label2id=tag2index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlmr_model = (XLMRobertaForTokenClassification\n              .from_pretrained(xlmr_model_name, config=xlmr_config)\n              .to(device))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = xlmr_model(input_ids.to(device)).logits\npredictions = torch.argmax(outputs, dim=-1)\nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\nprint(f\"Shape of outputs: {outputs.shape}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tag_text(text, tags, model, tokenizer):\n    # Get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # Encode the sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # Get predictions as distribution over 7 possible classes\n    outputs = model(input_ids)[0]\n    # Take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=2)\n    # Convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing Texts for NER","metadata":{}},{"cell_type":"code","source":"words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\npd.DataFrame([tokens], index=[\"Tokens\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nword_ids = tokenized_input.word_ids()\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nprevious_word_idx = None\nlabel_ids = []\n\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx])\n    previous_word_idx = word_idx\n    \nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n                                      is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, batched=True, \n                      remove_columns=['langs', 'ner_tags', 'tokens'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Measures","metadata":{}},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n          [\"B-PER\", \"I-PER\", \"O\"]]\nprint(classification_report(y_true, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n\n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n\n    return preds_list, labels_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning XLM-RoBERTa","metadata":{}},{"cell_type":"code","source":"# hide_output\nfrom transformers import TrainingArguments\n\nnum_epochs = 3\nbatch_size = 24\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\ntraining_args = TrainingArguments(\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n    logging_steps=logging_steps, push_to_hub=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, \n                                       eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_init():\n    return (XLMRobertaForTokenClassification\n            .from_pretrained(xlmr_model_name, config=xlmr_config)\n            .to(device))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide\n%env TOKENIZERS_PARALLELISM=false","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfrom transformers import Trainer\n\ntrainer = Trainer(model_init=model_init, args=training_args, \n                  data_collator=data_collator, compute_metrics=compute_metrics,\n                  train_dataset=panx_de_encoded[\"train\"],\n                  eval_dataset=panx_de_encoded[\"validation\"], \n                  tokenizer=xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\ndf = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss', 'eval_f1']]\ndf = df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", \"eval_f1\":\"F1\"})\ndf['Epoch'] = df[\"Epoch\"].apply(lambda x: round(x))\ndf['Training Loss'] = df[\"Training Loss\"].ffill()\ndf[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\ndf.drop_duplicates()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error Analysis","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # Convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    # Pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        # Pass data through model  \n        output = trainer.model(input_ids, attention_mask)\n        # Logit.size: [batch_size, sequence_length, classes]\n        # Predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    # Calculate loss per token after flattening batch dimension with view\n    loss = cross_entropy(output.logits.view(-1, 7), \n                         labels.view(-1), reduction=\"none\")\n    # Unflatten batch dimension and convert to numpy array\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n\n    return {\"loss\":loss, \"predicted_label\": predicted_label}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nvalid_set = panx_de_encoded[\"validation\"]\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = valid_set.to_pandas()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nindex2tag[-100] = \"IGN\"\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(\n    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf[\"labels\"] = df[\"labels\"].apply(\n    lambda x: [index2tag[i] for i in x])\ndf['loss'] = df.apply(\n    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\ndf['predicted_label'] = df.apply(\n    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\ndf.head(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndf_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\ndf_tokens.head(7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n    .sort_values(by=\"sum\", ascending=False)\n    .reset_index()\n    .round(2)\n    .head(10)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    df_tokens.groupby(\"labels\")[[\"loss\"]] \n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)\n    .sort_values(by=\"mean\", ascending=False)\n    .reset_index()\n    .round(2)\n    .T\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n                      tags.names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndef get_samples(df):\n    for _, row in df.iterrows():\n        labels, preds, tokens, losses = [], [], [], []\n        for i, mask in enumerate(row[\"attention_mask\"]):\n            if i not in {0, len(row[\"attention_mask\"])}:\n                labels.append(row[\"labels\"][i])\n                preds.append(row[\"predicted_label\"][i])\n                tokens.append(row[\"input_tokens\"][i])\n                losses.append(f\"{row['loss'][i]:.2f}\")\n        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n                               \"preds\": preds, \"losses\": losses}).T\n        yield df_tmp\n\ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ndf_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Lingual Transfer","metadata":{}},{"cell_type":"code","source":"def get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics[\"test_f1\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_fr = \"Jeff Dean est informaticien chez Google en Californie\"\ntag_text(text_fr, tags, trainer.model, xlmr_tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx_ch[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nf1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nf1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_input\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nf1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### When Does Zero-Shot Transfer Make Sense?","metadata":{}},{"cell_type":"code","source":"def train_on_subset(dataset, num_samples):\n    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n    valid_ds = dataset[\"validation\"]\n    test_ds = dataset[\"test\"]\n    training_args.logging_steps = len(train_ds) // batch_size\n    \n    trainer = Trainer(model_init=model_init, args=training_args,\n        data_collator=data_collator, compute_metrics=compute_metrics,\n        train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n    trainer.train()\n    if training_args.push_to_hub:\n        trainer.push_to_hub(commit_message=\"Training completed!\")\n    \n    f1_score = get_f1_score(trainer, test_ds)\n    return pd.DataFrame.from_dict(\n        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.push_to_hub = False\nmetrics_df = train_on_subset(panx_fr_encoded, 250)\nmetrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\n# Hack needed to exclude the progress bars in the above cell\nmetrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfor num_samples in [500, 1000, 2000, 4000]:\n    metrics_df = metrics_df.append(\n        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\nplt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\nplt.ylim((0, 1))\nplt.xlabel(\"Number of Training Samples\")\nplt.ylabel(\"F1 Score\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning on Multiple Languages at Once","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ndef concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets(\n            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\ntraining_args.push_to_hub = True\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n    eval_dataset=panx_de_fr_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_input\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ncorpora = [panx_de_encoded]\n\n# Exclude German from iteration\nfor lang in langs[1:]:\n    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n    # Fine-tune on monolingual corpus\n    ds_encoded = encode_panx_dataset(panx_ch[lang])\n    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n    # Collect F1-scores in common dict\n    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n    # Add monolingual corpus to list of corpora to concatenate\n    corpora.append(ds_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpora_encoded = concatenate_splits(corpora)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\ntraining_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n    data_collator=data_collator, compute_metrics=compute_metrics,\n    tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n    eval_dataset=corpora_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfor idx, lang in enumerate(langs):\n    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_data = {\"de\": f1_scores[\"de\"],\n               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n               \"all\": f1_scores[\"all\"]}\nf1_scores_df = pd.DataFrame(scores_data).T.round(4)\nf1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\",\n                         inplace=True)\nf1_scores_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interacting with Model Widgets","metadata":{}},{"cell_type":"markdown","source":"<img alt=\"A Hub widget\" caption=\"Example of a widget on the Hugging Face Hub\" src=\"images/chapter04_ner-widget.png\" id=\"ner-widget\"/>  ","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}